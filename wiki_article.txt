--[[User:Brandojazz|Brandojazz]] ([[User talk:Brandojazz|talk]]) 01:57, 8 December 2014 (UTC){{User sandbox}}
<!-- EDIT BELOW THIS LINE -->

==Uniform Stability and Generalization of a Learning Algorithm==

If a learning Algorithm <math>L </math> is uniformly stable as defined in the previous section, then it will generalize.
In other words, if the algorithm is resistant to small perturbations in the training set as defined by uniform stability,
then we can mathematically show that the algorithm will be able to predict well sample points that are inside our training set
as well as the samples not in it.

== Motivation ==

Why would someone want their learning algorithm to be stable in any sense?
One can intuitively motivate the concept of stability with the analogy of a scientific theory, where the power of our predictor lies
in the predictive power of the scientific theory.
If we had a good scientific theory and thus a good predictor, then it should have been able to see general trends in the data that
inspired this theory.
In other words, small changes to the data that inspired this theory, should not affect the theory as a whole to much, because the
theory was able to extract the general trends effectively in a predictive manner.
If it was a good theory in the first place, then small changes in the training data should not affect its success to predict unseen data samples.
Similarly, if we have a learning algorithm that is able to generalize on unseen data points, then if the algorithm was "good" at
learning from the data it had,
then small perturbations on that data should not affect its predictive power too much.
This intuitive idea can actually be made mathematically rigorous with uniform stability.
If a learning algorithm is (uniformly) stable then, from the previous argument, it makes sense that it should have good predictive power
on data it has already seen
and data it has not yet seen. i.e. if the learning algorithm is stable, it should be able to generalize.

== Statement ==

If a learning algorithm <math>L </math> is uniformly stable, then as the number of training points approaches infinity the empirical risk approaches the generalization error with high probability i.e.

<math> \lim_{n \to \infty} I_S[f_S] = I[f_S] </math>

with high probability.

Therefore, our goal will be to show that if uniform stability holds for <math>L </math>, then the difference between the empirical error and the generalization error will go to zero with high probability i.e.:

<math> Pr[ | I_S[f_S] - I[f_S] | \leq \epsilon(n) ] \geq 1 - \delta</math>

where <math> \epsilon(n)</math> will be an upper bound that approaches zero as n approaches infinity.

=== Proof ===

Let's begin the proof by using the fact that our learning algorithm is uniformly stable. If the learning algorithm<math> L </math> is uniformly stable, then using McDirmid's inequality, and setting the generalization error to be the functional yields:

<math>\forall (S, z) \in Z^{n+1}, \forall i \in \{1,...,n\}, \sup_{z' \in Z} | V(f_S , z' )-V(f_{S^{i,z} } , z' )|\leq\beta  \implies P( |I[f_S] - \mathbb{E}_S[f_S]|  \geq \epsilon ) \leq 2 e^{ \left( \frac{-2 \epsilon^2}{n \beta^2} \right) }</math>

Which says that the generalization error will be close to the expected generalization error over training sets with high probability. If we change the above probabilistic statement to its confidence form and demand to have <math> 1 - \delta </math> confidence that <math> I[f_S] </math> and <math> \mathbb{E}_S[f_S] </math> are close, then after some lines of algebra its easy to verify that <math> \epsilon </math> must be at most:

<math> e = n \beta \sqrt{\frac{ln(\frac{2}{\delta}) }{ 2n } } </math>

Therefore, we have with confidence <math> 1 - \delta </math> that:

<math> |I[f_S] - \mathbb{E}_S[f_S]|  \leq n \beta \sqrt{\frac{ln(\frac{2}{\delta}) }{ 2n } }  \implies I[f_S]  \leq \mathbb{E}_S[f_S]  +  n \beta \sqrt{\frac{ln(\frac{2}{\delta}) }{ 2n } } </math>

The above statement is an upper bound on the generalization error, which can be turned into the desired bound (i.e. one bounding the difference of the empirical risk and generalization error) easily by subtracting the empirical risk <math> I_S[f_S] </math> from both sides of the inequality yielding:

<math> I[f_S]  - I_S[f_S] \leq \mathbb{E}_S[f_S] - I_S[f_S]  +  n \beta \sqrt{\frac{ln(\frac{2}{\delta}) }{ 2n } } </math>

the LHS is exactly what we need to conclude the proof, however we need to finish upper bounding the RHS, specifically we need to upper bound <math> \mathbb{E}_S[f_S] - I_S[f_S] </math> to conclude the proof.

The upper bound for <math> \mathbb{E}_S[f_S] - I_S[f_S] </math> is exactly:

<math> \mathbb{E}_S[f_S] - I_S[f_S] \leq \beta + (n \beta + M) \sqrt{\frac{ln(\frac{2}{\delta}) }{ 2n } } </math>

where M is the upper on the loss function <math> V(f, z) </math>.

If that is true then the proof concludes that:

<math> I[f_S] - I_S[f_S] \leq \beta + (2n\beta + M) \sqrt{ \frac{ ln(   \frac{2}{\delta}  ) }{2n} }</math>

Which gives the desired upper bound and as long as the upper bound decreases as n increases, then the generalization error and empirical risk can be made arbitrarily close.

We will finish the proof by proving what we need in the following lemma:

==== Lemma ====

For a bounded loss function <math> V(f, z) </math> (with upper bound M) and a uniformly stable learning algorithm the following upper bound holds with confidence <math> 1 - \delta</math>

<math> \mathbb{E}_S[f_S] - I_S[f_S] \leq \beta + (n \beta + M) \sqrt{\frac{ln(\frac{2}{\delta}) }{ 2n } } </math>

===== Proof =====

Similarly as we used McDirmid's inequality in the previous proof, we will do it again but this time let the functional F in McDirmid's be <math> I_S[f_S] </math>.
If that is our choice of functional then we must show the following

<math> \forall i, \sup_{S, z} |I_S[f_S] - I_{S^{i,z} }[f_{S^{i,z}}]| \leq c_i</math>

if we want a probabilistic upper bound on <math> \mathbb{E}_S[I_S[f_S]] </math>.

Let's search for the upper bound on <math> c_i </math> by considering the LHS of the above inequality and expanding it:

<math> \forall i, \sup_{S, z} |I_S[f_S] - I_{S^{i,z} }[f_S^{i,z}]| = |\frac{1}{n}\sum^n_{i=1} V(f_s , z_i) - (\frac{1}{n} \sum_{j \neq i} V(f_{S^{i,z}} , z_j) + \frac{1}{n} V(f_{S^{i,z}},z) )   | </math>

by triangle inequality and pairing up the samples points <math> z_i \neq z_j </math> in the summations we get:

<math>  \leq \frac{1}{n} \sum_{j \neq i} |V(f_S, z_j) - V(f_{S^{i,z}} , z_j)| + \frac{1}{n} | V(f_S, z_i) - V(f_{S^{i,z}} , z ) |</math>

We divided the terms that way because the first term <math> V(f_S, z_j) - V(f_{S^{i,z}} , z_j) </math> can be upper bounded by <math> \beta </math> because of the stability of our learning algorithm. Thus,

<math> V(f_S, z_j) - V(f_{S^{i,z}} , z_j) \leq \beta</math>

and the second term can only be bounded by the the upper bound of the loss function (because we are evaluating the loss at two different points with different training points):

<math> V(f_S, z_i) - V(f_{S^{i,z}} , z) \leq M</math>

Combining both terms yields the desired upper bound (to use McDirmad's afterwards):

<math> \leq \frac{n-1}{n} \beta + \frac{M}{n}  \leq \beta + \frac{M}{n}</math>

With this last result we can apply McDirmid's inequality and the constants <math> c_i</math> at the beginning of the lemma are <math>\beta + \frac{M}{n} </math>:

<math> \forall i, \sup_{S, z} |I_S[f_S] - I_S^{{i,z}}[f_S^{i,z}]| \leq \beta + \frac{M}{n} \implies Pr[|I_S[f_S] - \mathbb{E}_S[  I_S[f_S]] | \geq \epsilon ] \leq 2 e^{\frac{-2n \epsilon^2}{(n\beta + M)^2} } </math>

Switching the above bound to its confidence form and requiring to have confidence <math> 1 - \delta </math> yields <math> \epsilon </math> to be:

<math> \epsilon = (n \beta + M) \sqrt{\frac{ln(   \frac{2}{\delta}   ) }{2n} }</math>

== Final Result ==

If a learning algorithm <math>L </math> is uniformly stable and also has a bounded loss function, then with confidence <math> 1 - \delta </math>  the difference between the empirical risk and the generalization error will be upper bounded as follows:

<math> I[f_S] - I_S[f_S] \leq \beta_n + (2n\beta_n + M) \sqrt{ \frac{ ln(   \frac{2}{\delta}  ) }{2n} }</math>

Specifically, if one has a uniformly stable learning algorithm with <math> \beta_n = O \left( \frac{1}{n} \right) </math>, then the upper bound becomes:

<math> I[f_S] - I_S[f_S] \leq O\left( \frac{ 1 }{ n } \right)</math>

which clearly generalizes as n approaches infinity because the difference between the generalization error and empirical risk goes to zero.
